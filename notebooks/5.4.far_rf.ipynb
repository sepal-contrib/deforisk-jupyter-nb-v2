{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93956c07-d9f1-4e3a-ae1b-148aec301334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "896caed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "# import pandas as pd\n",
    "# print(pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc67a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cuml\n",
    "# cuml.accel.install()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c599305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizations\n",
    "# GDAL optimizations\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "cpu_count: int = mp.cpu_count()\n",
    "num_cores: int = cpu_count - 2\n",
    "os.environ[\"GDAL_NUM_THREADS\"] = f\"{num_cores}\"\n",
    "os.environ[\"GDAL_CACHEMAX\"] = \"1024\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2a25b-11a9-436c-9eff-17ad4003f00c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f13a12a-7502-4c76-b490-9880fbf99c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import riskmapjnr as rmj\n",
    "from tabulate import tabulate\n",
    "from patsy import dmatrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd36889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root to path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from component.script.utilities.file_filter import (\n",
    "    list_files_by_extension,\n",
    "    filter_files_by_keywords,\n",
    "    filter_files_by_keywords_strict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15850cf5-6362-4e08-aef7-f9372ff90576",
   "metadata": {},
   "source": [
    "## Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "106736a8-74db-4767-b3c2-f7c398b97aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95608835",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_source = \"gfc\"  ##gfc, tmf\n",
    "tree_cover_threshold = 10\n",
    "years = [2015, 2020, 2024]\n",
    "string_years = [str(num) for num in years]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0e91e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_variables = [\"altitude\", \"slope\", \"pa\", \"subj\", \"dist_rivers\", \"dist_roads\"]\n",
    "dynamic_variables = [\"forest\", \"deforestation\", \"forest_edge\", \"dist_towns\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47987a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "n_trees = 100\n",
    "model_identifier_name = \"v1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb1bef",
   "metadata": {},
   "source": [
    "## Connect folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aba5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder: Path = Path.cwd().parent\n",
    "downloads_folder: Path = root_folder / \"data\"\n",
    "downloads_folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b029cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = downloads_folder / project_name\n",
    "project_folder.mkdir(parents=True, exist_ok=True)\n",
    "processed_data_folder = project_folder / \"data\"\n",
    "processed_data_folder.mkdir(parents=True, exist_ok=True)\n",
    "sampling_folder = project_folder / \"far_samples\"\n",
    "sampling_folder.mkdir(parents=True, exist_ok=True)\n",
    "rf_model_folder = project_folder / \"far_rf\"\n",
    "rf_model_folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade3d99",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcad33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_by_extension(folder_path, file_extensions, recursive=False):\n",
    "    files = []\n",
    "    for ext in file_extensions:\n",
    "        files.extend(\n",
    "            list(Path(folder_path).glob(f\"*{ext}\"))\n",
    "            if not recursive\n",
    "            else list(Path(folder_path).rglob(f\"*.{ext}\"))\n",
    "        )\n",
    "    files = [f for f in files if \".ipynb_checkpoints\" not in Path(f).parts]\n",
    "\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a6d94",
   "metadata": {},
   "source": [
    "## Select forest cover change file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e26396f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jose/workspace/deforisk-jupyter-nb-v2/data/test/data/test_defostack_gfc_10_2015_2020_2024_reprojected.tif')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all raster files in the processed data folder\n",
    "input_raster_files = list_files_by_extension(processed_data_folder, [\".tiff\", \".tif\"])\n",
    "forest_change_file = filter_files_by_keywords(input_raster_files, [\"defostack\"])[0]\n",
    "forest_change_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ce0fe",
   "metadata": {},
   "source": [
    "## Periods dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb5fe2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_full_period_dict(\n",
    "    years: list[int],\n",
    "    period: str,\n",
    "    processed_data_folder: Path,\n",
    "    static_variables: list[str],\n",
    "    dynamic_variables: list[str],\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a comprehensive dictionary for a given modeling period.\n",
    "    Handles period-independent and multi-temporal variables separately.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(years) < 3:\n",
    "        raise ValueError(\"The 'years' list must contain at least three elements.\")\n",
    "\n",
    "    configs = {\n",
    "        \"calibration\": {\n",
    "            \"train_period\": \"calibration\",\n",
    "            \"initial_idx\": 0,\n",
    "            \"final_idx\": 1,\n",
    "            \"defor_value\": 1,\n",
    "            \"var_idx\": 0,\n",
    "        },\n",
    "        \"validation\": {\n",
    "            \"train_period\": \"calibration\",\n",
    "            \"initial_idx\": 1,\n",
    "            \"final_idx\": 2,\n",
    "            \"defor_value\": 1,\n",
    "            \"var_idx\": 1,\n",
    "        },\n",
    "        \"historical\": {\n",
    "            \"train_period\": \"historical\",\n",
    "            \"initial_idx\": 0,\n",
    "            \"final_idx\": 2,\n",
    "            \"defor_value\": [1, 2],\n",
    "            \"var_idx\": 0,\n",
    "        },\n",
    "        \"forecast\": {\n",
    "            \"train_period\": \"historical\",\n",
    "            \"initial_idx\": 0,\n",
    "            \"final_idx\": 2,\n",
    "            \"defor_value\": [1, 2],\n",
    "            \"var_idx\": 2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if period not in configs:\n",
    "        raise ValueError(f\"Unknown period '{period}'. Must be one of: {list(configs)}.\")\n",
    "\n",
    "    c = configs[period]\n",
    "\n",
    "    # --- Base period dictionary ---\n",
    "    period_dict = {\n",
    "        \"period\": period,\n",
    "        \"train_period\": c[\"train_period\"],\n",
    "        \"initial_year\": years[c[\"initial_idx\"]],\n",
    "        \"final_year\": years[c[\"final_idx\"]],\n",
    "        \"defor_value\": c[\"defor_value\"],\n",
    "        \"time_interval\": years[c[\"final_idx\"]] - years[c[\"initial_idx\"]],\n",
    "        \"var_year\": years[c[\"var_idx\"]],\n",
    "    }\n",
    "\n",
    "    initial_year = str(period_dict[\"initial_year\"])\n",
    "    final_year = str(period_dict[\"final_year\"])\n",
    "    var_year = str(period_dict[\"var_year\"])\n",
    "    exclude_years = \", \".join(map(str, set(years) - {initial_year, final_year}))\n",
    "    period_name = str(period_dict[\"period\"])\n",
    "\n",
    "    variable_file_mapping = {\"period\": period}\n",
    "    input_raster_files = list_files_by_extension(\n",
    "        processed_data_folder, [\".tiff\", \".tif\"]\n",
    "    )\n",
    "\n",
    "    # --- Modular file search ---\n",
    "    def _is_token_separate_in_name(token: str, name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Devuelve True si `token` aparece en `name` como 'palabra' separada por\n",
    "        caracteres no alfanum√©ricos o en los l√≠mites (comportamiento similar a \\b,\n",
    "        pero \\b considera \"_\" como no palabra; aqu√≠ queremos lo mismo).\n",
    "        \"\"\"\n",
    "        if token.isdigit():  # a√±os u otros n√∫meros: buscar la secuencia directamente\n",
    "            return token in name\n",
    "        # construimos regex que asegura token no est√° pegado a letras o n√∫meros\n",
    "        pattern = rf\"(?<![0-9A-Za-z]){re.escape(token)}(?![0-9A-Za-z])\"\n",
    "        return re.search(pattern, name) is not None\n",
    "\n",
    "    def _strict_candidate_filter(candidates, tokens):\n",
    "        \"\"\"\n",
    "        Filtra candidatos manteniendo s√≥lo aquellos que contienen todos los tokens\n",
    "        como 'palabras' separadas (ver _is_token_separate_in_name).\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        for p in candidates:\n",
    "            s = str(p).lower()\n",
    "            if all(_is_token_separate_in_name(tok.lower(), s) for tok in tokens):\n",
    "                filtered.append(p)\n",
    "        return filtered\n",
    "\n",
    "    def find_file(var_name, dynamic=False):\n",
    "        \"\"\"\n",
    "        Busca un archivo que contenga los t√©rminos relevantes.\n",
    "        Si es din√°mico, incluye los a√±os del periodo.\n",
    "        \"\"\"\n",
    "        parts = var_name.split(\"_\")\n",
    "        include_terms = []\n",
    "        if len(parts) == 1:\n",
    "            exclude_terms = [\"distance\", \"edge\"]\n",
    "        else:\n",
    "            exclude_terms = None\n",
    "\n",
    "        if dynamic:\n",
    "            if period_name != \"forecast\":\n",
    "                # Buscar archivos que incluyan los a√±os del periodo\n",
    "                if \"deforestation\" in parts:\n",
    "                    include_terms = [*parts, initial_year, final_year]\n",
    "                else:\n",
    "                    include_terms = [*parts, initial_year]\n",
    "            elif period_name == \"forecast\":\n",
    "                include_terms = [*parts, var_year]\n",
    "        else:\n",
    "            include_terms = parts\n",
    "\n",
    "        # Buscar distancias o bordes si el nombre lo indica\n",
    "        if \"dist\" in parts and \"distance\" not in include_terms:\n",
    "            include_terms.append(\"distance\")\n",
    "\n",
    "        files = filter_files_by_keywords(\n",
    "            input_raster_files, include_terms, False, exclude_terms, True\n",
    "        )\n",
    "        # Si no hay archivos, devolvemos None\n",
    "        if not files and period_name == \"forecast\":\n",
    "            include_terms = [*parts, str(years[1])]\n",
    "            files = filter_files_by_keywords(\n",
    "                input_raster_files, include_terms, False, exclude_terms, True\n",
    "            )\n",
    "        if not files:\n",
    "            return None\n",
    "\n",
    "        # Si viene solo 1, ok\n",
    "        if len(files) == 1:\n",
    "            return files[0]\n",
    "        strict = _strict_candidate_filter(files, parts)\n",
    "        if strict:\n",
    "            # si hay m√∫ltiplos a√∫n, devolvemos el primero (heur√≠stica)\n",
    "            return strict[0]\n",
    "\n",
    "    # --- Buscar variables independientes ---\n",
    "    for var in static_variables:\n",
    "        variable_file_mapping[var] = find_file(var, dynamic=False)\n",
    "\n",
    "    # --- Buscar variables multitemporales ---\n",
    "    for var in dynamic_variables:\n",
    "        variable_file_mapping[var] = find_file(var, dynamic=True)\n",
    "\n",
    "    # --- Merge final ---\n",
    "    period_dict.update(variable_file_mapping)\n",
    "    return period_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1b3e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_dict = create_full_period_dict(\n",
    "    years,\n",
    "    \"calibration\",\n",
    "    processed_data_folder,\n",
    "    static_variables,\n",
    "    dynamic_variables,\n",
    ")\n",
    "validation_dict = create_full_period_dict(\n",
    "    years,\n",
    "    \"validation\",\n",
    "    processed_data_folder,\n",
    "    static_variables,\n",
    "    dynamic_variables,\n",
    ")\n",
    "historical_dict = create_full_period_dict(\n",
    "    years,\n",
    "    \"historical\",\n",
    "    processed_data_folder,\n",
    "    static_variables,\n",
    "    dynamic_variables,\n",
    ")\n",
    "forecast_dict = create_full_period_dict(\n",
    "    years,\n",
    "    \"forecast\",\n",
    "    processed_data_folder,\n",
    "    static_variables,\n",
    "    dynamic_variables,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81c4f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el diccionario principal\n",
    "period_dictionaries = {\n",
    "    calibration_dict[\"period\"]: calibration_dict,\n",
    "    validation_dict[\"period\"]: validation_dict,\n",
    "    historical_dict[\"period\"]: historical_dict,\n",
    "    forecast_dict[\"period\"]: forecast_dict,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09885629",
   "metadata": {},
   "source": [
    "## Training formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85205cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_formula = \"I(1-fcc) + trial ~ scale(altitude) + scale(dist_edge) + scale(dist_river) + scale(dist_road) + scale(dist_town) + scale(slope) + C(pa)\"\n",
    "user_formula = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4baccb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from component.script.far_helpers import generate_patsy_formula\n",
    "\n",
    "\n",
    "dependant_variable = \"1-deforestation\"\n",
    "independent_variables_continuous = [\n",
    "    \"altitude\",\n",
    "    \"forest_edge\",\n",
    "    \"dist_rivers\",\n",
    "    \"dist_roads\",\n",
    "    \"dist_towns\",\n",
    "    \"slope\",\n",
    "]\n",
    "independant_variable_categorical = [\"pa\"]\n",
    "\n",
    "calculated_formula = generate_patsy_formula(\n",
    "    dependant_variable,\n",
    "    independent_variables_continuous,\n",
    "    independant_variable_categorical,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a88ced01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I(1-deforestation) + trial ~ scale(altitude) + scale(forest_edge) + scale(dist_rivers) + scale(dist_roads) + scale(dist_towns) + scale(slope) + C(pa)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if user_formula is None:\n",
    "    training_formula = calculated_formula\n",
    "elif user_formula is not None:\n",
    "    training_formula = user_formula\n",
    "training_formula\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ab15b-f327-4d50-9bfd-c38a1b3051a4",
   "metadata": {},
   "source": [
    "## Train rf based on period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51e50b65-d558-4ead-9271-6a15c2452996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from component.script.far_helpers import extract_variables\n",
    "\n",
    "\n",
    "def train_rf_from_formula(\n",
    "    formula: str,\n",
    "    dataset_file: str,\n",
    "    out_file: str = \"rf_model.pickle\",\n",
    "    random_state: int = 42,\n",
    "    n_estimators: int = 50,\n",
    "    min_samples_leaf: int = 2,\n",
    "    max_depth: int = 15,\n",
    "    n_jobs: int = -1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier from a formula and dataset.\n",
    "    Preprocessing:\n",
    "        - Reads CSV file\n",
    "        - Drops rows with missing values\n",
    "        - Adds 'trial' = 1\n",
    "        - Filters dataset to only include variables used in the formula\n",
    "        - Validates required columns exist\n",
    "\n",
    "    Parameters:\n",
    "        formula (str): Patsy-style formula (e.g., 'target ~ var1 + C(var2)')\n",
    "        dataset (pd.DataFrame): Input data\n",
    "        out_file (str): Path to save the trained model via joblib\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        n_estimators (int): Number of trees in forest\n",
    "        min_samples_leaf (int): Minimum samples per leaf node\n",
    "        max_depth (int): Max depth of each tree\n",
    "        n_jobs (int): Number of parallel jobs; set to 1 for QGIS safety\n",
    "        verbose (bool): Whether to print progress\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with model, predictions, and deviance\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the dataset from the text file\n",
    "    print(f\"üìä Loading data from {dataset_file}...\")\n",
    "    try:\n",
    "        dataset = pd.read_csv(dataset_file)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to read dataset file: {e}\")\n",
    "\n",
    "    if dataset.empty:\n",
    "        raise ValueError(\"Dataset is empty after loading.\")\n",
    "\n",
    "    # Apply required preprocessing\n",
    "    print(\"üßπ Preprocessing data: dropping missing values and adding 'trial' column...\")\n",
    "    dataset = dataset.dropna(axis=0)  # Drop any rows with NA\n",
    "    dataset[\"trial\"] = 1  # Add trial column as 1\n",
    "\n",
    "    # Extract raw variable names used in the formula (ignoring I(), scale(), C())\n",
    "    raw_variables = extract_variables(formula, \"all\")\n",
    "\n",
    "    # Also ensure that `trial` and `cell` are present ‚Äî these are often used as offsets or weights\n",
    "    required_vars = raw_variables | {\"trial\"}\n",
    "\n",
    "    # Check which required variables are missing from dataset\n",
    "    missing_vars = [var for var in required_vars if var not in dataset.columns]\n",
    "\n",
    "    if missing_vars:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_vars}\")\n",
    "\n",
    "    print(f\"Variablesrequired in the formula {raw_variables}\")\n",
    "\n",
    "    # Now filter the dataset: keep only relevant columns\n",
    "    try:\n",
    "        dataset = dataset[list(required_vars)]\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"Failed to select columns from dataset: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"üíæ Filtered dataset to {len(dataset.columns)} variables: {list(dataset.columns)}\"\n",
    "    )\n",
    "\n",
    "    # Parse formula using patsy\n",
    "    y, x = dmatrices(formula, data=dataset, NA_action=\"drop\")\n",
    "    # Ensure consistent preprocessing\n",
    "    # Debug: Confirm alignment\n",
    "    if len(y) != len(x):\n",
    "        raise ValueError(\n",
    "            f\"Inconsistent sample sizes after dmatrices: Y={len(y)}, X={len(x)}\"\n",
    "        )\n",
    "\n",
    "    Y = y[:, 0]\n",
    "    X = x\n",
    "    # X = x[:, 1:-1]\n",
    "\n",
    "    # Remove intercept (first column) and spatial cell ID (last column)\n",
    "\n",
    "    # Fit Random Forest model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    # Predictions\n",
    "    pred_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Compute deviance (twice the log loss)\n",
    "    deviance = 2 * log_loss(Y, pred_proba, normalize=False)\n",
    "\n",
    "    # Save model metadata (pickle)\n",
    "\n",
    "    model_data = {\n",
    "        \"model\": model,\n",
    "        \"predictions\": pred_proba,\n",
    "        \"deviance\": deviance,\n",
    "        \"formula\": formula,\n",
    "        \"dataset_shape\": dataset.shape,\n",
    "        \"samples_path\": dataset_file,\n",
    "    }\n",
    "\n",
    "    # Save model with joblib\n",
    "    # joblib.dump(model_data, out_file, compress=3)\n",
    "    # Save model with pickle\n",
    "    with open(out_file, \"wb\") as file:\n",
    "        pickle.dump(model_data, file)\n",
    "\n",
    "    print(f\"‚úÖ Random Forest trained and saved to: {out_file}\")\n",
    "    return model_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3d6d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_for_period(period, sample_name: str = \"sample.txt\"):\n",
    "    period_name = period_dictionaries[period][\"train_period\"]\n",
    "    samples = sampling_folder / period_name / sample_name\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc47d7d6-c1d4-47cd-a1e6-562ca540aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_period(formula, period, sample_path, random_seed, n_estimators, model_id):\n",
    "    # Create period folder\n",
    "    period_output_folder = rf_model_folder / period\n",
    "    period_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set outputfile\n",
    "    model_output = period_output_folder / f\"rf_model_{model_id}.pickle\"\n",
    "    # Train RF\n",
    "    rf_trined = train_rf_from_formula(\n",
    "        formula, sample_path, model_output, random_seed, n_estimators\n",
    "    )\n",
    "    return model_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8e0a128-ac72-4e9e-9ac0-2fb93b6e50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading data from /home/jose/workspace/deforisk-jupyter-nb-v2/data/test/far_samples/calibration/sample.txt...\n",
      "üßπ Preprocessing data: dropping missing values and adding 'trial' column...\n",
      "Variablesrequired in the formula {'slope', 'deforestation', 'altitude', 'pa', 'dist_rivers', 'forest_edge', 'dist_towns', 'trial', 'dist_roads'}\n",
      "üíæ Filtered dataset to 9 variables: ['slope', 'deforestation', 'altitude', 'pa', 'dist_rivers', 'forest_edge', 'dist_towns', 'trial', 'dist_roads']\n",
      "‚úÖ Random Forest trained and saved to: /home/jose/workspace/deforisk-jupyter-nb-v2/data/test/far_rf/calibration/rf_model_v1.pickle\n"
     ]
    }
   ],
   "source": [
    "# Train calibration period\n",
    "period_t = \"calibration\"\n",
    "\n",
    "samples = get_samples_for_period(period_t, \"sample.txt\")\n",
    "\n",
    "rf_trined_calibration = train_rf_period(\n",
    "    training_formula, period_t, samples, random_seed, n_trees, model_identifier_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3335805d-510c-43fe-a76b-e38078460f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading data from /home/jose/workspace/deforisk-jupyter-nb-v2/data/test/far_samples/historical/sample.txt...\n",
      "üßπ Preprocessing data: dropping missing values and adding 'trial' column...\n",
      "Variablesrequired in the formula {'slope', 'deforestation', 'altitude', 'pa', 'dist_rivers', 'forest_edge', 'dist_towns', 'trial', 'dist_roads'}\n",
      "üíæ Filtered dataset to 9 variables: ['slope', 'deforestation', 'altitude', 'pa', 'dist_rivers', 'forest_edge', 'dist_towns', 'trial', 'dist_roads']\n",
      "‚úÖ Random Forest trained and saved to: /home/jose/workspace/deforisk-jupyter-nb-v2/data/test/far_rf/historical/rf_model_v1.pickle\n"
     ]
    }
   ],
   "source": [
    "# Train calibration period\n",
    "period_t = \"historical\"\n",
    "\n",
    "samples = get_samples_for_period(period_t, \"sample.txt\")\n",
    "\n",
    "rf_trined_historical = train_rf_period(\n",
    "    training_formula, period_t, samples, random_seed, n_trees, model_identifier_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1cc56",
   "metadata": {},
   "source": [
    "## Apply rf based on period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standard library imports\n",
    "# import os\n",
    "# import sys\n",
    "# import uuid\n",
    "\n",
    "# # Third party imports\n",
    "# import numpy as np\n",
    "# from osgeo import gdal\n",
    "# import pandas as pd\n",
    "# from patsy.build import build_design_matrices\n",
    "\n",
    "# # Local application imports\n",
    "# from forestatrisk.misc import rescale, makeblock\n",
    "\n",
    "\n",
    "# # predict_raster\n",
    "# def predict_raster(\n",
    "#     model,\n",
    "#     _x_design_info,\n",
    "#     period_dict_files=\"data\",\n",
    "#     input_forest_raster=\"data/forest.tif\",\n",
    "#     output_file=\"predictions.tif\",\n",
    "#     blk_rows=128,\n",
    "#     verbose=True,\n",
    "# ):\n",
    "#     \"\"\"Predict the spatial probability of deforestation from a\n",
    "#     statistical model.\n",
    "\n",
    "#     This function predicts the spatial probability of deforestation\n",
    "#     from a statistical model. Computation are done by block and\n",
    "#     can be performed on large geographical areas.\n",
    "\n",
    "#     :param model: The model (glm, rf) to predict from. Must have a\n",
    "#         model.predict_proba() function.\n",
    "#     :param _x_design_info: Design matrix information from patsy.\n",
    "#     :param var_dir: Directory with rasters (.tif) of explicative variables.\n",
    "#     :param input_forest_raster: Path to forest raster (1 for forest).\n",
    "#     :param output_file: Name of the output raster file for predictions.\n",
    "#     :param blk_rows: If > 0, number of rows for computation by block.\n",
    "#     :param verbose: Logical. Whether to print messages or not. Default\n",
    "#         to ``True``.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Mask on forest\n",
    "#     if verbose:\n",
    "#         print(f\"Using {input_forest_raster} file\")\n",
    "#     fmaskR = gdal.Open(input_forest_raster)\n",
    "#     fmaskB = fmaskR.GetRasterBand(1)\n",
    "\n",
    "#     # Landscape variables from forest raster\n",
    "#     gt = fmaskR.GetGeoTransform()\n",
    "#     ncol = fmaskR.RasterXSize\n",
    "#     nrow = fmaskR.RasterYSize\n",
    "#     Xmin = gt[0]\n",
    "#     Xmax = gt[0] + gt[1] * ncol\n",
    "#     Ymin = gt[3] + gt[5] * nrow\n",
    "#     Ymax = gt[3]\n",
    "\n",
    "#     # Raster list\n",
    "#     # Extract keys excluding 'fcc', 'forest' and 'period' and sort them\n",
    "#     sorted_keys = sorted(\n",
    "#         [key for key in period_dict_files.keys() if key not in [\"period\", \"forest\"]]\n",
    "#     )\n",
    "\n",
    "#     # Retrieve the corresponding file paths based on the sorted keys\n",
    "#     raster_list = [period_dict_files[key] for key in sorted_keys]\n",
    "#     # raster_names = []\n",
    "#     # for i in range(len(raster_list)):\n",
    "#     #     fname = os.path.basename(raster_list[i])\n",
    "#     #     index_dot = fname.index(\".\")\n",
    "#     #     raster_names.append(fname[:index_dot])\n",
    "#     var_names = sorted_keys\n",
    "#     var_names.extend([\"X\", \"Y\", \"fmask\"])\n",
    "#     # print(len(var_names), len(raster_list))\n",
    "#     # Make vrt with gdalbuildvrt\n",
    "#     if verbose:\n",
    "#         print(\"Make virtual raster with variables as raster bands\")\n",
    "#     param = gdal.BuildVRTOptions(\n",
    "#         resolution=\"user\",\n",
    "#         outputBounds=(Xmin, Ymin, Xmax, Ymax),\n",
    "#         xRes=gt[1],\n",
    "#         yRes=-gt[5],\n",
    "#         separate=True,\n",
    "#     )\n",
    "#     rand_uuid = uuid.uuid4()\n",
    "#     vrt_file = f\"/vsimem/var_{rand_uuid}.vrt\"\n",
    "#     cback = gdal.TermProgress_nocb if verbose else 0\n",
    "#     gdal.BuildVRT(vrt_file, raster_list, options=param, callback=cback)\n",
    "#     stack = gdal.Open(vrt_file)\n",
    "#     nband = stack.RasterCount\n",
    "#     proj = stack.GetProjection()\n",
    "\n",
    "#     # List of nodata values\n",
    "#     bandND = np.zeros(nband)\n",
    "#     for k in range(nband):\n",
    "#         band = stack.GetRasterBand(k + 1)\n",
    "#         bandND[k] = band.GetNoDataValue()\n",
    "#         if (bandND[k] is None) or (bandND[k] is np.nan):\n",
    "#             print(f\"NoData value is not specified for input raster file {k}\")\n",
    "#             sys.exit(1)\n",
    "#     bandND = bandND.astype(np.float32)\n",
    "\n",
    "#     # Make blocks\n",
    "#     blockinfo = makeblock(vrt_file, blk_rows=blk_rows)\n",
    "#     nblock = blockinfo[0]\n",
    "#     nblock_x = blockinfo[1]\n",
    "#     x = blockinfo[3]\n",
    "#     y = blockinfo[4]\n",
    "#     nx = blockinfo[5]\n",
    "#     ny = blockinfo[6]\n",
    "#     if verbose:\n",
    "#         print(f\"Divide region in {nblock} blocks\")\n",
    "\n",
    "#     # Raster of predictions\n",
    "#     if verbose:\n",
    "#         print(\"Create a raster file on disk for projections\")\n",
    "#     driver = gdal.GetDriverByName(\"GTiff\")\n",
    "#     try:\n",
    "#         os.remove(output_file)\n",
    "#     except FileNotFoundError:\n",
    "#         pass\n",
    "#     Pdrv = driver.Create(\n",
    "#         output_file,\n",
    "#         ncol,\n",
    "#         nrow,\n",
    "#         1,\n",
    "#         gdal.GDT_UInt16,\n",
    "#         [\"COMPRESS=DEFLATE\", \"PREDICTOR=2\", \"BIGTIFF=YES\"],\n",
    "#     )\n",
    "#     Pdrv.SetGeoTransform(gt)\n",
    "#     Pdrv.SetProjection(proj)\n",
    "#     Pband = Pdrv.GetRasterBand(1)\n",
    "#     Pband.SetNoDataValue(0)\n",
    "\n",
    "#     # Predict by block\n",
    "#     # Message\n",
    "#     if verbose:\n",
    "#         print(\"Predict deforestation probability by block\")\n",
    "#     # Loop on blocks of data\n",
    "#     for b in range(nblock):\n",
    "#         # Position in 1D-arrays\n",
    "#         px = b % nblock_x\n",
    "#         py = b // nblock_x\n",
    "#         # Number of pixels\n",
    "#         npix = nx[px] * ny[py]\n",
    "#         # Data for one block of the stack (shape = (nband, nrow, ncol))\n",
    "#         data = stack.ReadAsArray(x[px], y[py], nx[px], ny[py])\n",
    "#         data = data.astype(float)  # From uint to float\n",
    "#         # Replace ND values with -9999\n",
    "#         for i in range(nband):\n",
    "#             data[i][np.nonzero(data[i] == bandND[i])] = -9999\n",
    "#         # Add a dimension if there is only one variable\n",
    "#         if len(data.shape) == 2:\n",
    "#             data = data[np.newaxis, :, :]\n",
    "#         # Coordinates of the center of the pixels of the block\n",
    "#         X_col = (\n",
    "#             gt[0] + x[px] * gt[1] + (np.arange(nx[px]) + 0.5) * gt[1]\n",
    "#         )  # +0.5 for center of pixels\n",
    "#         X = np.repeat(X_col[np.newaxis, :], ny[py], axis=0)\n",
    "#         X = X[np.newaxis, :, :]\n",
    "#         Y_row = (\n",
    "#             gt[3] + y[py] * gt[5] + (np.arange(ny[py]) + 0.5) * gt[5]\n",
    "#         )  # +0.5 for center of pixels\n",
    "#         Y = np.repeat(Y_row[:, np.newaxis], nx[px], axis=1)\n",
    "#         Y = Y[np.newaxis, :, :]\n",
    "#         # Forest mask\n",
    "#         fmaskA = fmaskB.ReadAsArray(x[px], y[py], nx[px], ny[py])\n",
    "#         fmaskA = fmaskA.astype(float)  # From uint to float\n",
    "#         fmaskA[np.nonzero(fmaskA != 1)] = -9999\n",
    "#         fmaskA = fmaskA[np.newaxis, :, :]\n",
    "#         # Concatenate forest mask with stack\n",
    "#         data = np.concatenate((data, X, Y, fmaskA), axis=0)\n",
    "#         # Transpose and reshape to 2D array\n",
    "#         data = data.transpose(1, 2, 0)\n",
    "#         data = data.reshape(npix, nband + 3)\n",
    "#         # Observations without NA\n",
    "#         w = np.nonzero(~(data == -9999).any(axis=1))\n",
    "#         # Remove observations with NA\n",
    "#         data = data[w]\n",
    "#         # Transform into a pandas DataFrame\n",
    "#         df = pd.DataFrame(data)\n",
    "#         df.columns = var_names\n",
    "#         # Add fake cell column for _x_design_info\n",
    "#         df[\"cell\"] = 0\n",
    "#         # Predict\n",
    "#         pred = np.zeros(npix)  # Initialize with nodata value (0)\n",
    "#         if len(w[0]) > 0:\n",
    "#             # Get X\n",
    "#             (x_new,) = build_design_matrices([_x_design_info], df)\n",
    "#             X_new = x_new  # [:, :-1]\n",
    "#             # if \"LogisticRegression\" in str(model):\n",
    "#             #     X_new = x_new[:, :-1]\n",
    "#             # else:\n",
    "#             #     X_new = x_new[:, 1:-1]\n",
    "#             # Get predictions into an array\n",
    "#             p = model.predict_proba(X_new)[:, 1]\n",
    "#             # Rescale and return to pred\n",
    "#             pred[w] = rescale(p)\n",
    "#         # Assign prediction to raster\n",
    "#         pred = pred.reshape(ny[py], nx[px])\n",
    "#         Pband.WriteArray(pred, x[px], y[py])\n",
    "\n",
    "#     # Compute statistics\n",
    "#     if verbose:\n",
    "#         print(\"Compute statistics\")\n",
    "#     Pband.FlushCache()  # Write cache data to disk\n",
    "#     Pband.ComputeStatistics(False)\n",
    "\n",
    "#     # Dereference driver\n",
    "#     Pband = None\n",
    "#     del Pdrv\n",
    "\n",
    "\n",
    "# # End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c18a5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from patsy.build import build_design_matrices\n",
    "\n",
    "# Local application imports\n",
    "from forestatrisk.misc import rescale, makeblock\n",
    "\n",
    "\n",
    "# predict_raster\n",
    "def predict_raster(\n",
    "    model_pickle,\n",
    "    _x_design_info,\n",
    "    period_dict=\"data\",\n",
    "    output_file=\"predictions.tif\",\n",
    "    blk_rows=128,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Predict the spatial probability of deforestation from a\n",
    "    statistical model.\n",
    "\n",
    "    This function predicts the spatial probability of deforestation\n",
    "    from a statistical model. Computation are done by block and\n",
    "    can be performed on large geographical areas.\n",
    "\n",
    "    :param model: The model (glm, rf) to predict from. Must have a\n",
    "        model.predict_proba() function.\n",
    "    :param _x_design_info: Design matrix information from patsy.\n",
    "    :param var_dir: Directory with rasters (.tif) of explicative variables.\n",
    "    :param output_file: Name of the output raster file for predictions.\n",
    "    :param blk_rows: If > 0, number of rows for computation by block.\n",
    "    :param verbose: Logical. Whether to print messages or not. Default\n",
    "        to ``True``.\n",
    "\n",
    "    \"\"\"\n",
    "    # Read model and extract data\n",
    "    model_pickle = pd.read_pickle(model_pickle)\n",
    "    model = model_pickle.get(\"model\")\n",
    "    formula = model_pickle.get(\"formula\")\n",
    "    predictors_variable = sorted(extract_variables(formula, \"predictors\"))\n",
    "\n",
    "    # Retrieve the corresponding file paths based on the sorted keys\n",
    "    raster_list = [period_dict[key] for key in predictors_variable]\n",
    "\n",
    "    # Get forest layer from period dictionary\n",
    "    input_forest_raster = period_dict[\"forest\"]\n",
    "\n",
    "    # Mask on forest\n",
    "    if verbose:\n",
    "        print(f\"Using {input_forest_raster} file\")\n",
    "    fmaskR = gdal.Open(input_forest_raster)\n",
    "    fmaskB = fmaskR.GetRasterBand(1)\n",
    "\n",
    "    # Landscape variables from forest raster\n",
    "    gt = fmaskR.GetGeoTransform()\n",
    "    ncol = fmaskR.RasterXSize\n",
    "    nrow = fmaskR.RasterYSize\n",
    "    Xmin = gt[0]\n",
    "    Xmax = gt[0] + gt[1] * ncol\n",
    "    Ymin = gt[3] + gt[5] * nrow\n",
    "    Ymax = gt[3]\n",
    "\n",
    "    # raster_names = []\n",
    "    # for i in range(len(raster_list)):\n",
    "    #     fname = os.path.basename(raster_list[i])\n",
    "    #     index_dot = fname.index(\".\")\n",
    "    #     raster_names.append(fname[:index_dot])\n",
    "    var_names = predictors_variable\n",
    "    var_names.extend([\"X\", \"Y\", \"fmask\"])\n",
    "    # print(len(var_names), len(raster_list))\n",
    "    # Make vrt with gdalbuildvrt\n",
    "    if verbose:\n",
    "        print(\"Make virtual raster with variables as raster bands\")\n",
    "    param = gdal.BuildVRTOptions(\n",
    "        resolution=\"user\",\n",
    "        outputBounds=(Xmin, Ymin, Xmax, Ymax),\n",
    "        xRes=gt[1],\n",
    "        yRes=-gt[5],\n",
    "        separate=True,\n",
    "    )\n",
    "    rand_uuid = uuid.uuid4()\n",
    "    vrt_file = f\"/vsimem/var_{rand_uuid}.vrt\"\n",
    "    cback = gdal.TermProgress_nocb if verbose else 0\n",
    "    gdal.BuildVRT(vrt_file, raster_list, options=param, callback=cback)\n",
    "    stack = gdal.Open(vrt_file)\n",
    "    nband = stack.RasterCount\n",
    "    proj = stack.GetProjection()\n",
    "\n",
    "    # List of nodata values\n",
    "    bandND = np.zeros(nband)\n",
    "    for k in range(nband):\n",
    "        band = stack.GetRasterBand(k + 1)\n",
    "        bandND[k] = band.GetNoDataValue()\n",
    "        if (bandND[k] is None) or (bandND[k] is np.nan):\n",
    "            print(f\"NoData value is not specified for input raster file {k}\")\n",
    "            sys.exit(1)\n",
    "    bandND = bandND.astype(np.float32)\n",
    "\n",
    "    # Make blocks\n",
    "    blockinfo = makeblock(vrt_file, blk_rows=blk_rows)\n",
    "    nblock = blockinfo[0]\n",
    "    nblock_x = blockinfo[1]\n",
    "    x = blockinfo[3]\n",
    "    y = blockinfo[4]\n",
    "    nx = blockinfo[5]\n",
    "    ny = blockinfo[6]\n",
    "    if verbose:\n",
    "        print(f\"Divide region in {nblock} blocks\")\n",
    "\n",
    "    # Raster of predictions\n",
    "    if verbose:\n",
    "        print(\"Create a raster file on disk for projections\")\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    try:\n",
    "        os.remove(output_file)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    Pdrv = driver.Create(\n",
    "        output_file,\n",
    "        ncol,\n",
    "        nrow,\n",
    "        1,\n",
    "        gdal.GDT_UInt16,\n",
    "        [\"COMPRESS=DEFLATE\", \"PREDICTOR=2\", \"BIGTIFF=YES\"],\n",
    "    )\n",
    "    Pdrv.SetGeoTransform(gt)\n",
    "    Pdrv.SetProjection(proj)\n",
    "    Pband = Pdrv.GetRasterBand(1)\n",
    "    Pband.SetNoDataValue(0)\n",
    "\n",
    "    # Predict by block\n",
    "    # Message\n",
    "    if verbose:\n",
    "        print(\"Predict deforestation probability by block\")\n",
    "    # Loop on blocks of data\n",
    "    for b in range(nblock):\n",
    "        # Position in 1D-arrays\n",
    "        px = b % nblock_x\n",
    "        py = b // nblock_x\n",
    "        # Number of pixels\n",
    "        npix = nx[px] * ny[py]\n",
    "        # Data for one block of the stack (shape = (nband, nrow, ncol))\n",
    "        data = stack.ReadAsArray(x[px], y[py], nx[px], ny[py])\n",
    "        data = data.astype(float)  # From uint to float\n",
    "        # Replace ND values with -9999\n",
    "        for i in range(nband):\n",
    "            data[i][np.nonzero(data[i] == bandND[i])] = -9999\n",
    "        # Add a dimension if there is only one variable\n",
    "        if len(data.shape) == 2:\n",
    "            data = data[np.newaxis, :, :]\n",
    "        # Coordinates of the center of the pixels of the block\n",
    "        X_col = (\n",
    "            gt[0] + x[px] * gt[1] + (np.arange(nx[px]) + 0.5) * gt[1]\n",
    "        )  # +0.5 for center of pixels\n",
    "        X = np.repeat(X_col[np.newaxis, :], ny[py], axis=0)\n",
    "        X = X[np.newaxis, :, :]\n",
    "        Y_row = (\n",
    "            gt[3] + y[py] * gt[5] + (np.arange(ny[py]) + 0.5) * gt[5]\n",
    "        )  # +0.5 for center of pixels\n",
    "        Y = np.repeat(Y_row[:, np.newaxis], nx[px], axis=1)\n",
    "        Y = Y[np.newaxis, :, :]\n",
    "        # Forest mask\n",
    "        fmaskA = fmaskB.ReadAsArray(x[px], y[py], nx[px], ny[py])\n",
    "        fmaskA = fmaskA.astype(float)  # From uint to float\n",
    "        fmaskA[np.nonzero(fmaskA != 1)] = -9999\n",
    "        fmaskA = fmaskA[np.newaxis, :, :]\n",
    "        # Concatenate forest mask with stack\n",
    "        data = np.concatenate((data, X, Y, fmaskA), axis=0)\n",
    "        # Transpose and reshape to 2D array\n",
    "        data = data.transpose(1, 2, 0)\n",
    "        data = data.reshape(npix, nband + 3)\n",
    "        # Observations without NA\n",
    "        w = np.nonzero(~(data == -9999).any(axis=1))\n",
    "        # Remove observations with NA\n",
    "        data = data[w]\n",
    "        # Transform into a pandas DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        df.columns = var_names\n",
    "        # Add fake cell column for _x_design_info\n",
    "        df[\"cell\"] = 0\n",
    "        # Predict\n",
    "        pred = np.zeros(npix)  # Initialize with nodata value (0)\n",
    "        if len(w[0]) > 0:\n",
    "            # Get X\n",
    "            (x_new,) = build_design_matrices([_x_design_info], df)\n",
    "            X_new = x_new  # [:, :-1]\n",
    "            # if \"LogisticRegression\" in str(model):\n",
    "            #     X_new = x_new[:, :-1]\n",
    "            # else:\n",
    "            #     X_new = x_new[:, 1:-1]\n",
    "            # Get predictions into an array\n",
    "            p = model.predict_proba(X_new)[:, 1]\n",
    "            # Rescale and return to pred\n",
    "            pred[w] = rescale(p)\n",
    "        # Assign prediction to raster\n",
    "        pred = pred.reshape(ny[py], nx[px])\n",
    "        Pband.WriteArray(pred, x[px], y[py])\n",
    "\n",
    "    # Compute statistics\n",
    "    if verbose:\n",
    "        print(\"Compute statistics\")\n",
    "    Pband.FlushCache()  # Write cache data to disk\n",
    "    Pband.ComputeStatistics(False)\n",
    "\n",
    "    # Dereference driver\n",
    "    Pband = None\n",
    "    del Pdrv\n",
    "\n",
    "\n",
    "# End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0df09a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from patsy import dmatrices\n",
    "import forestatrisk\n",
    "from component.script.far_helpers import get_design_info\n",
    "\n",
    "\n",
    "def apply_rf_period(\n",
    "    period_dictionaries,\n",
    "    period,\n",
    "    model,\n",
    "):\n",
    "    period_dictionary = period_dictionaries[period]\n",
    "    period_output_folder = rf_model_folder / period\n",
    "    period_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    prediction_output = (\n",
    "        period_output_folder / f\"rf_{period}_{model_identifier_name}.tif\"\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model_f = pd.read_pickle(model)\n",
    "    formula = model_f.get(\"formula\")\n",
    "    samples = model_f.get(\"samples_path\")\n",
    "    (y_design_info, x_design_info) = get_design_info(formula, samples)\n",
    "    time_interval = period_dictionary[\"time_interval\"]\n",
    "\n",
    "    predict_raster(\n",
    "        model,\n",
    "        x_design_info,\n",
    "        period_dictionary,\n",
    "        prediction_output,\n",
    "        blk_rows=256,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # defrate_per_cat\n",
    "    print(\"Calculate deforestation rate per cathegory\")\n",
    "    defrate_output = str(\n",
    "        period_output_folder\n",
    "        / f\"defrate_cat_glm_{period_dictionary['period']}_{model_identifier_name}.csv\"\n",
    "    )\n",
    "    forestatrisk.defrate_per_cat(\n",
    "        forest_change_file,\n",
    "        str(prediction_output),\n",
    "        time_interval,\n",
    "        period,\n",
    "        defrate_output,\n",
    "        256,\n",
    "        False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "965c813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(period_dictionaries, period, model_name):\n",
    "    period_name = period_dictionaries[period][\"train_period\"]\n",
    "    model_period_folder = rf_model_folder / period_name\n",
    "    model = model_period_folder / model_name\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34533e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/jose/workspace/deforisk-jupyter-nb-v2/data/test/data/test_forest_gfc_10_2015_reprojected.tif file\n",
      "Make virtual raster with variables as raster bands\n",
      "Divide region in 32 blocks\n",
      "Create a raster file on disk for projections\n",
      "Predict deforestation probability by block\n",
      "Compute statistics\n",
      "Calculate deforestation rate per cathegory\n"
     ]
    }
   ],
   "source": [
    "# Predict over calibration period\n",
    "\n",
    "period_c = \"calibration\"\n",
    "\n",
    "model = get_trained_model(\n",
    "    period_dictionaries, period_c, f\"rf_model_{model_identifier_name}.pickle\"\n",
    ")\n",
    "\n",
    "rf_predict_calibration = apply_rf_period(\n",
    "    period_dictionaries,\n",
    "    period_c,\n",
    "    model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bc03d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/jose/workspace/deforisk-jupyter-nb-v2/data/test/data/test_forest_gfc_10_2020_reprojected.tif file\n",
      "Make virtual raster with variables as raster bands\n",
      "Divide region in 32 blocks\n",
      "Create a raster file on disk for projections\n",
      "Predict deforestation probability by block\n",
      "Compute statistics\n",
      "Calculate deforestation rate per cathegory\n"
     ]
    }
   ],
   "source": [
    "period_c = \"validation\"\n",
    "\n",
    "model = get_trained_model(\n",
    "    period_dictionaries, period_c, f\"rf_model_{model_identifier_name}.pickle\"\n",
    ")\n",
    "\n",
    "rf_predict_validation = apply_rf_period(\n",
    "    period_dictionaries,\n",
    "    period_c,\n",
    "    model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d580553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/jose/workspace/deforisk-jupyter-nb-v2/data/test/data/test_forest_gfc_10_2015_reprojected.tif file\n",
      "Make virtual raster with variables as raster bands\n",
      "Divide region in 32 blocks\n",
      "Create a raster file on disk for projections\n",
      "Predict deforestation probability by block\n",
      "Compute statistics\n",
      "Calculate deforestation rate per cathegory\n"
     ]
    }
   ],
   "source": [
    "period_c = \"historical\"\n",
    "\n",
    "model = get_trained_model(\n",
    "    period_dictionaries, period_c, f\"rf_model_{model_identifier_name}.pickle\"\n",
    ")\n",
    "\n",
    "rf_predict_validation = apply_rf_period(\n",
    "    period_dictionaries,\n",
    "    period_c,\n",
    "    model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79cc072d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/jose/workspace/deforisk-jupyter-nb-v2/data/test/data/test_forest_gfc_10_2024_reprojected.tif file\n",
      "Make virtual raster with variables as raster bands\n",
      "Divide region in 32 blocks\n",
      "Create a raster file on disk for projections\n",
      "Predict deforestation probability by block\n",
      "Compute statistics\n",
      "Calculate deforestation rate per cathegory\n"
     ]
    }
   ],
   "source": [
    "period_c = \"forecast\"\n",
    "\n",
    "model = get_trained_model(\n",
    "    period_dictionaries, period_c, f\"rf_model_{model_identifier_name}.pickle\"\n",
    ")\n",
    "\n",
    "rf_predict_forecast = apply_rf_period(\n",
    "    period_dictionaries,\n",
    "    period_c,\n",
    "    model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadae862",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deforisk-jupyter-nb (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
